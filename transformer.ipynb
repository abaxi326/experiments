{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d3634a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import keras\n",
    "\n",
    "# Hyperparameters\n",
    "num_heads = 4\n",
    "key_dim = 64\n",
    "ff_dim = 256\n",
    "input_vocab_size = 1000\n",
    "target_vocab_size = 1000\n",
    "max_len = 50\n",
    "embed_dim = num_heads * key_dim  # Must match num_heads * key_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a9e22fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(layers.Layer):\n",
    "    def __init__(self,num_heads,key_dim,ff_dim):\n",
    "        super().__init__()\n",
    "        self.attn=layers.MultiHeadAttention(num_heads,key_dim)\n",
    "        self.ffn=keras.Sequential([\n",
    "            layers.Dense(ff_dim,activation='relu'),\n",
    "            layers.Dense(embed_dim)\n",
    "        ])\n",
    "        self.norm1=layers.LayerNormalization()\n",
    "        self.norm2=layers.LayerNormalization()\n",
    "        self.dropout1 = layers.Dropout(0.1)\n",
    "        self.dropout2 = layers.Dropout(0.1)\n",
    "    \n",
    "    def call(self,inputs):\n",
    "        attn_out=self.attn(inputs,inputs)\n",
    "        x1=self.norm1(attn_out+inputs)\n",
    "        x2=self.ffn(x1)\n",
    "        out=self.norm2(x1+x2)\n",
    "        return out\n",
    "encoder=Encoder(num_heads,key_dim,ff_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "55b2a63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.random.random(size=(20,80,256))\n",
    "y=np.random.randint(0,1000,size=(20,80))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "87eaf3b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 20, 256), dtype=float32, numpy=\n",
       "array([[[-0.649084  ,  1.0761093 , -0.52660125, ...,  0.26298004,\n",
       "          0.06401635, -1.8109443 ],\n",
       "        [ 0.1479908 ,  0.89725554,  0.4548781 , ..., -0.49070308,\n",
       "         -0.7247118 , -1.5038882 ],\n",
       "        [-0.18679012, -0.61070246,  0.5352549 , ..., -0.34154418,\n",
       "         -0.21008793,  0.84406525],\n",
       "        ...,\n",
       "        [ 0.35534573,  0.05886101, -0.12305738, ..., -0.6793685 ,\n",
       "         -0.33180696,  0.18863481],\n",
       "        [ 0.56061625, -1.1070285 , -0.2135569 , ...,  0.3142345 ,\n",
       "          0.03185473,  0.05495362],\n",
       "        [-0.8543104 ,  0.376284  ,  0.585863  , ..., -2.0291612 ,\n",
       "          0.187307  , -0.5297316 ]]], dtype=float32)>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input=np.random.random(size=(1,20,64*4))\n",
    "encoder(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "58bd653f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(layers.Layer):\n",
    "    def __init__(self,num_heads,key_dim,ff_dim):\n",
    "        super().__init__()\n",
    "        self.attn1=layers.MultiHeadAttention(num_heads,key_dim)\n",
    "        self.attn2=layers.MultiHeadAttention(num_heads,key_dim)\n",
    "        self.norm1=layers.LayerNormalization()\n",
    "        self.norm2=layers.LayerNormalization()\n",
    "        self.norm3=layers.LayerNormalization()\n",
    "        self.ffn=keras.Sequential([\n",
    "            layers.Dense(ff_dim,activation='relu'),\n",
    "            layers.Dense(embed_dim)\n",
    "        ])\n",
    "        self.final=layers.Dense(1000)\n",
    "\n",
    "    def call(self,inputs,encoder_out):\n",
    "        attn1=self.attn1(inputs,inputs)\n",
    "        norm1=self.norm1(attn1+inputs)\n",
    "        attn2=self.attn2(inputs,encoder_out,encoder_out)\n",
    "        norm2=self.norm2(attn2+norm1)\n",
    "        ffn=self.ffn(norm2)\n",
    "        norm3=self.norm3(ffn+norm2)\n",
    "        out=self.final(norm3)\n",
    "        return out\n",
    "decoder=Decoder(num_heads,key_dim,ff_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e5944483",
   "metadata": {},
   "outputs": [],
   "source": [
    "output=np.random.random(size=(1,20,64*4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5e36ba62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 20, 256)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1d4aff48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 20, 1000), dtype=float32, numpy=\n",
       "array([[[ 0.46030194,  0.2590694 ,  1.1781337 , ...,  0.67232203,\n",
       "          0.21596876, -0.88003135],\n",
       "        [ 0.48037672,  0.1798131 ,  0.53229946, ..., -1.506984  ,\n",
       "          1.6327589 ,  0.08765519],\n",
       "        [-0.34970826, -0.17879637,  0.86428535, ...,  0.19337237,\n",
       "         -0.76814914,  0.457581  ],\n",
       "        ...,\n",
       "        [ 0.6966606 , -0.62689054,  0.76753   , ...,  0.914107  ,\n",
       "          0.66620576, -0.67119473],\n",
       "        [-0.42851877,  0.5160184 ,  0.3727733 , ..., -0.84890187,\n",
       "          1.3006573 ,  0.10884823],\n",
       "        [ 0.47528273, -0.12156303, -0.06013846, ..., -0.832541  ,\n",
       "         -0.06490389, -0.61822474]]], dtype=float32)>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder(input,input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8bea9571",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "class Transformer(keras.Model):\n",
    "    def __init__(self,encoder,decoder):\n",
    "        super().__init__()\n",
    "        self.encoder=encoder\n",
    "        self.decoder=decoder\n",
    "        \n",
    "    def call(self,inputs):\n",
    "        en_out=self.encoder(inputs)\n",
    "        dec_out=self.decoder(inputs,en_out)\n",
    "        return dec_out\n",
    "    \n",
    "    def train_step(self,data):\n",
    "        x,y=data\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred=self(x)\n",
    "            loss=self.compiled_loss(y,y_pred)\n",
    "        train_vars=self.trainable_variables\n",
    "        grads=tape.gradient(loss,train_vars)\n",
    "        self.optimizer.apply_gradients(zip(grads,train_vars))\n",
    "        self.compiled_metrics.update_state(y, y_pred)\n",
    "\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "31bd6bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Transformer(encoder,decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3b2c4f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=keras.losses.SparseCategoricalCrossentropy(),optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "41782704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - loss: -0.4207\n",
      "Epoch 2/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - loss: -0.4427\n",
      "Epoch 3/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - loss: -0.4638\n",
      "Epoch 4/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - loss: -0.4790\n",
      "Epoch 5/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - loss: -0.4929\n",
      "Epoch 6/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - loss: -0.4933\n",
      "Epoch 7/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - loss: -0.4929\n",
      "Epoch 8/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - loss: -0.4906\n",
      "Epoch 9/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - loss: -0.4973\n",
      "Epoch 10/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - loss: -0.5073\n",
      "Epoch 11/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - loss: -0.5201\n",
      "Epoch 12/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - loss: -0.5343\n",
      "Epoch 13/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - loss: -0.5525\n",
      "Epoch 14/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - loss: -0.5710\n",
      "Epoch 15/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step - loss: -0.6125\n",
      "Epoch 16/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - loss: -0.6313\n",
      "Epoch 17/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - loss: -0.6235\n",
      "Epoch 18/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - loss: -0.6309\n",
      "Epoch 19/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - loss: -0.6450\n",
      "Epoch 20/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step - loss: -0.6813\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x18adc1be5d0>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x,y,epochs=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
